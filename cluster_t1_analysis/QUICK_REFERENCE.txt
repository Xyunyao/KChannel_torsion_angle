╔══════════════════════════════════════════════════════════════════════════╗
║                  T1 ANISOTROPY HPC CLUSTER - QUICK REFERENCE             ║
╚══════════════════════════════════════════════════════════════════════════╝

📦 PACKAGE: cluster_t1_analysis.tar.gz (127 MB)
🔬 ANALYSIS: 13C T1 relaxation from 4μs MD trajectory  
🧬 SYSTEM: KcsA Channel, Chain A, Residues 22-120 (99 jobs)

┌──────────────────────────────────────────────────────────────────────────┐
│ TRANSFER TO CLUSTER                                                      │
└──────────────────────────────────────────────────────────────────────────┘

  # Use /local/$USER or /tmp/$USER depending on your cluster
  scp cluster_t1_analysis.tar.gz user@hpc:/local/$USER/
  ssh user@hpc
  cd /local/$USER      # Or cd /tmp/$USER
  tar -xzf cluster_t1_analysis.tar.gz
  cd cluster_t1_analysis

┌──────────────────────────────────────────────────────────────────────────┐
│ SETUP ENVIRONMENT (ONE TIME)                                             │
└──────────────────────────────────────────────────────────────────────────┘

  module load anaconda3
  conda create -n kcsa_torsion python=3.8
  conda activate kcsa_torsion
  pip install -r requirements.txt

┌──────────────────────────────────────────────────────────────────────────┐
│ TEST BEFORE SUBMITTING (CRITICAL!)                                       │
└──────────────────────────────────────────────────────────────────────────┘

  ./test_local.sh 22
  
  ✅ Should complete in ~30s-2min
  ✅ Should print T1_ensemble value
  ✅ Creates test_results/residue_22/

┌──────────────────────────────────────────────────────────────────────────┐
│ SUBMIT JOBS                                                              │
└──────────────────────────────────────────────────────────────────────────┘

  ./submit_all.sh cpu      # For CPU partition (fcpu)
  ./submit_all.sh gpu      # For GPU partition (fgpu)

  ➜ Submits 99 parallel jobs (one per residue)
  ➜ Auto-submits collection job after completion

┌──────────────────────────────────────────────────────────────────────────┐
│ MONITOR JOBS                                                             │
└──────────────────────────────────────────────────────────────────────────┘

  squeue -u $USER                           # View all your jobs
  squeue -u $USER -t RUNNING                # Running jobs only
  tail -f logs/t1_res_22.out                # Watch progress
  ls results/residue_*/ensemble*.png | wc -l   # Count completed (should be 99)

┌──────────────────────────────────────────────────────────────────────────┐
│ CANCEL JOBS (IF NEEDED)                                                  │
└──────────────────────────────────────────────────────────────────────────┘

  scancel -u $USER                          # Cancel all your jobs
  scancel <JOB_ID>                         # Cancel specific job

┌──────────────────────────────────────────────────────────────────────────┐
│ CHECK RESULTS                                                            │
└──────────────────────────────────────────────────────────────────────────┘

  cat results/ensemble_t1_summary.csv      # T1 values for all residues
  ls results/residue_*/ensemble*.png       # Individual residue plots
  grep "T1_ensemble =" logs/*.out          # Extract all T1 values

┌──────────────────────────────────────────────────────────────────────────┐
│ DOWNLOAD RESULTS                                                         │
└──────────────────────────────────────────────────────────────────────────┘

  # On your local machine (adjust path: /local/$USER or /tmp/$USER):
  scp -r user@hpc:/local/$USER/cluster_t1_analysis/results ./
  scp user@hpc:/local/$USER/cluster_t1_analysis/logs/*.out ./logs/

┌──────────────────────────────────────────────────────────────────────────┐
│ EXPECTED TIMELINE                                                        │
└──────────────────────────────────────────────────────────────────────────┘

  Transfer:        5-10 minutes
  Setup:           5 minutes
  Test:            30 seconds
  Queue wait:      Varies (minutes to hours)
  Execution:       30-60 minutes (parallel)
  Total:           ~1-2 hours typically

┌──────────────────────────────────────────────────────────────────────────┐
│ SUCCESS INDICATORS                                                       │
└──────────────────────────────────────────────────────────────────────────┘

  ✅ 99 result directories in results/
  ✅ ensemble_t1_summary.csv with 100 lines (1 header + 99 data)
  ✅ ensemble_t1_vs_residue.png plot created
  ✅ No errors in logs/*.err files

┌──────────────────────────────────────────────────────────────────────────┐
│ TROUBLESHOOTING                                                          │
└──────────────────────────────────────────────────────────────────────────┘

  Jobs fail immediately:
    → Check logs/t1_res_22.err
    → Verify conda environment: conda list | grep numpy
    → Run ./test_local.sh 22

  Jobs timeout:
    → Edit submit_t1_array.slurm
    → Increase: #SBATCH --time=01:00:00

  Out of memory:
    → Edit submit_t1_array.slurm
    → Increase: #SBATCH --mem=8G

  Some residues fail:
    → Check individual logs: cat logs/t1_res_XX.err
    → May have missing atoms (skip those residues)

┌──────────────────────────────────────────────────────────────────────────┐
│ KEY FILES                                                                │
└──────────────────────────────────────────────────────────────────────────┘

  📋 README.md           - Complete documentation
  ✅ CHECKLIST.md        - Step-by-step guide
  📄 PACKAGE_SUMMARY.md  - Overview & quick start
  🚀 submit_all.sh       - Submit jobs
  🔬 submit_t1_array.slurm  - Job array script
  📊 collect_results.slurm  - Post-processing
  🧪 test_local.sh       - Test locally

┌──────────────────────────────────────────────────────────────────────────┐
│ CUSTOMIZATION                                                            │
└──────────────────────────────────────────────────────────────────────────┘

  Different chains:
    Edit submit_t1_array.slurm, line 45: CHAIN="B"

  Different residue range:
    Edit submit_t1_array.slurm, line 6: #SBATCH --array=50-100

  Cluster-specific:
    Edit partition name, module loads, conda activation method

┌──────────────────────────────────────────────────────────────────────────┐
│ ANALYSIS PARAMETERS                                                      │
└──────────────────────────────────────────────────────────────────────────┘

  dt:         1e-12 s    (1 ps time step)
  max_lag:    2000       (2 ns correlation window)  
  lag_step:   1          (⚠️  DO NOT CHANGE - critical for accuracy)
  B0:         14.1 T     (magnetic field strength)
  Chain:      A          (modify for B, C, D)
  Residues:   22-120     (99 residues)

╔══════════════════════════════════════════════════════════════════════════╗
║  QUICK START: Transfer → Setup → Test → Submit → Monitor → Download     ║
║  Full docs: README.md | Step-by-step: CHECKLIST.md                       ║
╚══════════════════════════════════════════════════════════════════════════╝
